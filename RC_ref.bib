
@article{tanaka_recent_2019,
	title = {Recent advances in physical reservoir computing: {A} review},
	volume = {115},
	issn = {0893-6080},
	shorttitle = {Recent advances in physical reservoir computing},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608019300784},
	doi = {10.1016/j.neunet.2019.03.005},
	abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
	urldate = {2023-11-09},
	journal = {Neural Networks},
	author = {Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
	month = jul,
	year = {2019},
	keywords = {Machine learning, Neural networks, Neuromorphic device, Nonlinear dynamical systems, Reservoir computing},
	pages = {100--123},
	file = {ScienceDirect Snapshot:/Users/niharkoppolu/Zotero/storage/H3RNP8XB/S0893608019300784.html:text/html;Submitted Version:/Users/niharkoppolu/Zotero/storage/NZ3VKZQ5/Tanaka et al. - 2019 - Recent advances in physical reservoir computing A.pdf:application/pdf},
}

@inproceedings{trouvain_reservoirpy_2020,
	title = {{ReservoirPy}: an {Efficient} and {User}-{Friendly} {Library} to {Design} {Echo} {State} {Networks}},
	shorttitle = {{ReservoirPy}},
	url = {https://inria.hal.science/hal-02595026},
	abstract = {We present a simple user-friendly library called ReservoirPy based on Python scientific modules. It provides a flexible interface to implement efficient Reservoir Computing (RC) architectures with a particular focus on Echo State Networks (ESN). Advanced features of ReservoirPy allow to improve up to 87.9\% of computation time efficiency on a simple laptop compared to basic Python implementation. Overall, we provide tutorials for hyperparameters tuning, offline and online training, fast spectral initialization, parallel and sparse matrix computation on various tasks (MackeyGlass and audio recognition tasks). In particular, we provide graphical tools to easily explore hyperparameters using random search with the help of the hyperopt library.},
	language = {en},
	urldate = {2023-11-09},
	author = {Trouvain, Nathan and Pedrelli, Luca and Dinh, Thanh Trung and Hinaut, Xavier},
	month = sep,
	year = {2020},
	file = {Full Text PDF:/Users/niharkoppolu/Zotero/storage/GY8RCQA9/Trouvain et al. - 2020 - ReservoirPy an Efficient and User-Friendly Librar.pdf:application/pdf},
}

@inproceedings{fonnegra_performance_2017,
	title = {Performance comparison of deep learning frameworks in image classification problems using convolutional and recurrent networks},
	url = {https://ieeexplore.ieee.org/abstract/document/8088219},
	doi = {10.1109/ColComCon.2017.8088219},
	abstract = {Deep learning methods have resulted in effective strategies for improving performance in a large number of applications, becoming one of the most used strategies by developers and researchers. In order to facilitate the implementation of those approaches, a set of software frameworks have been developed and are currently available. Selection of a specific framework is an important task, especially when computational resources are limited. In order to provide information for deciding it, this paper presents a comparative study of three of the most widely used deep learning frameworks namely Tensorflow, Theano and Torch. The comparison is carried out by implementing convolutional and recurrent architectures for classifying images from two databases: MNIST and aFAR-10. Computational costs i.e. gradient computation time, forward time and memory consumption are reported for both GPU and CPU settings.},
	urldate = {2023-11-09},
	booktitle = {2017 {IEEE} {Colombian} {Conference} on {Communications} and {Computing} ({COLCOM})},
	author = {Fonnegra, Rubén D. and Blair, Bryan and Díaz, Gloria M.},
	month = aug,
	year = {2017},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/niharkoppolu/Zotero/storage/9MUZIJXH/8088219.html:text/html;IEEE Xplore Full Text PDF:/Users/niharkoppolu/Zotero/storage/5262VA4V/Fonnegra et al. - 2017 - Performance comparison of deep learning frameworks.pdf:application/pdf},
}

@article{cucchi_hands-reservoir_2022,
	title = {Hands-on reservoir computing: a tutorial for practical implementation},
	volume = {2},
	issn = {2634-4386},
	shorttitle = {Hands-on reservoir computing},
	url = {https://dx.doi.org/10.1088/2634-4386/ac7db7},
	doi = {10.1088/2634-4386/ac7db7},
	abstract = {This manuscript serves a specific purpose: to give readers from fields such as material science, chemistry, or electronics an overview of implementing a reservoir computing (RC) experiment with her/his material system. Introductory literature on the topic is rare and the vast majority of reviews puts forth the basics of RC taking for granted concepts that may be nontrivial to someone unfamiliar with the machine learning field (see for example reference Lukoševičius (2012 Neural Networks: Tricks of the Trade (Berlin: Springer) pp 659–686). This is unfortunate considering the large pool of material systems that show nonlinear behavior and short-term memory that may be harnessed to design novel computational paradigms. RC offers a framework for computing with material systems that circumvents typical problems that arise when implementing traditional, fully fledged feedforward neural networks on hardware, such as minimal device-to-device variability and control over each unit/neuron and connection. Instead, one can use a random, untrained reservoir where only the output layer is optimized, for example, with linear regression. In the following, we will highlight the potential of RC for hardware-based neural networks, the advantages over more traditional approaches, and the obstacles to overcome for their implementation. Preparing a high-dimensional nonlinear system as a well-performing reservoir for a specific task is not as easy as it seems at first sight. We hope this tutorial will lower the barrier for scientists attempting to exploit their nonlinear systems for computational tasks typically carried out in the fields of machine learning and artificial intelligence. A simulation tool to accompany this paper is available online 7 .},
	language = {en},
	number = {3},
	urldate = {2023-11-09},
	journal = {Neuromorphic Computing and Engineering},
	author = {Cucchi, Matteo and Abreu, Steven and Ciccone, Giuseppe and Brunner, Daniel and Kleemann, Hans},
	month = aug,
	year = {2022},
	note = {Publisher: IOP Publishing},
	pages = {032002},
	file = {IOP Full Text PDF:/Users/niharkoppolu/Zotero/storage/E2C7WHMX/Cucchi et al. - 2022 - Hands-on reservoir computing a tutorial for pract.pdf:application/pdf},
}

@article{medsker_recurrent_2001,
	title = {Recurrent neural networks},
	volume = {5},
	number = {64-67},
	journal = {Design and Applications},
	author = {Medsker, Larry R and Jain, LC},
	year = {2001},
	pages = {2},
}

@article{jaeger2001short,
  title={Short term memory in echo state networks},
  author={Jaeger, Herbert},
  year={2001},
  publisher={GMD Forschungszentrum Informationstechnik}
}

@article{shahi2022prediction,
  title={Prediction of chaotic time series using recurrent neural networks and reservoir computing techniques: A comparative study},
  author={Shahi, Shahrokh and Fenton, Flavio H and Cherry, Elizabeth M},
  journal={Machine learning with applications},
  volume={8},
  pages={100300},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{schrauwen2007overview,
  title={An overview of reservoir computing: theory, applications and implementations},
  author={Schrauwen, Benjamin and Verstraeten, David and Van Campenhout, Jan},
  booktitle={Proceedings of the 15th european symposium on artificial neural networks. p. 471-482 2007},
  pages={471--482},
  year={2007}
}

@article{deckers2022extended,
  title={Extended liquid state machines for speech recognition},
  author={Deckers, Lucas and Tsang, Ing Jyh and Van Leekwijck, Werner and Latr{\'e}, Steven},
  journal={Frontiers in Neuroscience},
  volume={16},
  pages={1023470},
  year={2022},
  publisher={Frontiers}
}

@article{zheng2020long,
  title={Long-short term echo state network for time series prediction},
  author={Zheng, Kaihong and Qian, Bin and Li, Sen and Xiao, Yong and Zhuang, Wanqing and Ma, Qianli},
  journal={IEEE Access},
  volume={8},
  pages={91961--91974},
  year={2020},
  publisher={IEEE}
}

@article{jaeger2007echo,
  title={Echo state network},
  author={Jaeger, Herbert},
  journal={scholarpedia},
  volume={2},
  number={9},
  pages={2330},
  year={2007}
}

@incollection{Trouvain2020,
  doi = {10.1007/978-3-030-61616-8_40},
  url = {https://doi.org/10.1007/978-3-030-61616-8_40},
  year = {2020},
  publisher = {Springer International Publishing},
  pages = {494--505},
  author = {Nathan Trouvain and Luca Pedrelli and Thanh Trung Dinh and Xavier Hinaut},
  title = {{ReservoirPy}: An Efficient and User-Friendly Library to Design Echo State Networks},
  booktitle = {Artificial Neural Networks and Machine Learning {\textendash} {ICANN} 2020}
}

@inproceedings{parvat2017survey,
  title={A survey of deep-learning frameworks},
  author={Parvat, Aniruddha and Chavan, Jai and Kadam, Siddhesh and Dev, Souradeep and Pathak, Vidhi},
  booktitle={2017 International Conference on Inventive Systems and Control (ICISC)},
  pages={1--7},
  year={2017},
  organization={IEEE}
}


@misc{steiner2021pyrcn,
      title={PyRCN: A Toolbox for Exploration and Application of Reservoir Computing Networks}, 
      author={Peter Steiner and Azarakhsh Jalalvand and Simon Stone and Peter Birkholz},
      year={2021},
      eprint={2103.04807},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{joy2022rctorch,
  title={RcTorch: a PyTorch Reservoir Computing Package with Automated Hyper-Parameter Optimization},
  author={Joy, Hayden and Mattheakis, Marios and Protopapas, Pavlos},
  journal={arXiv preprint arXiv:2207.05870},
  year={2022}
}

@article{JMLR:v23:22-0611,
  author  = {Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora},
  title   = {ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {288},
  pages   = {1--8},
  url     = {http://jmlr.org/papers/v23/22-0611.html}
}